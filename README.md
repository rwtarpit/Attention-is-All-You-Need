# Attention-is-All-You-Need
Implementing the ground-breaking paper 'Attention is all you need' from scratch using only Pytorch

This repo contains a from-scratch PyTorch implementation of the legendary paper "Attention Is All You Need" by Vaswani et al. (2017), which introduced the Transformer architecture to the world. Spoiler alert: it changed everything.

ðŸš€ Features
Encoder and Decoder architecture âœ…

Multi-head self-attention mechanism âœ…

Positional Encoding âœ…

Masked Attention for autoregressive decoding âœ…

Training loop with teacher forcing âœ…


As of now i have implemented the structure of transformer and the training loop.
just to make sure everything is oe the mark, i overfitted the model with 2 samples only. CrossEntropyLoss < 1 gives strong reason to believe everything is going smooth ATM. Inference loop will also be implemented soon.

I will try to train the model on a appropriate dataset once I get good GPU support.
